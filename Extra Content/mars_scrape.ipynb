{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pymongo\n",
    "import time\n",
    "import pandas as pd\n",
    "from splinter import Browser\n",
    "from splinter.exceptions import ElementDoesNotExist\n",
    "import requests\n",
    "import os\n",
    "import datetime as dt\n",
    "\n",
    "def init_browser():\n",
    "    # @NOTE: Replace the path with your actual path to the chromedriver\n",
    "    executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "    return Browser(\"chrome\", **executable_path, headless=False)\n",
    "\n",
    "def scrape_info():\n",
    "    \n",
    "    browser = init_browser()\n",
    "\n",
    "    # URLs of page to be scraped\n",
    "    news_url = 'https://mars.nasa.gov/news'\n",
    "    photo_url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "    weather_url = 'https://twitter.com/marswxreport?lang=en'\n",
    "    facts_url = 'https://space-facts.com/mars/'\n",
    "    usgs_url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "    short_photo_url = 'https://www.jpl.nasa.gov'\n",
    "    \n",
    "    #get current date and time\n",
    "    pulldate = dt.datetime.now()\n",
    "    pulldate = pulldate.strftime(\"%d %B %Y %I:%M:%S %p\")\n",
    "    \n",
    "    # create mars dictionary\n",
    "    mars_dict = {}\n",
    "    \n",
    "    mars_dict['pulldate'] = pulldate\n",
    "\n",
    "    # Retrieve page with the requests module\n",
    "    response = requests.get(news_url)\n",
    "    # Create BeautifulSoup object; parse with 'html'\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    ########retrieve articlce title and desc\n",
    "    news_title = soup.find('div',class_=\"content_title\").text\n",
    "    news_desc = soup.find('div',class_=\"rollover_description_inner\").text\n",
    "\n",
    "    mars_dict['news_title'] = news_title\n",
    "    mars_dict['news_desc'] = news_desc\n",
    "\n",
    "    ######## use splinter to get featured photo\n",
    "    browser.visit(photo_url)\n",
    "    # sleep\n",
    "    time.sleep(3)\n",
    "    # get html\n",
    "    html = browser.html\n",
    "    # parse HTML with Beautiful Soup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    # click on the featured photo\n",
    "    browser.find_by_id('full_image').click()\n",
    "    # sleep\n",
    "    time.sleep(3)\n",
    "    # get soup object with link embeded \n",
    "    featured_image_url = soup.find('a',class_=\"button fancybox\").attrs\n",
    "    # concat dynamic link\n",
    "    featured_image_url = short_photo_url + featured_image_url[\"data-fancybox-href\"]\n",
    "\n",
    "    mars_dict['featured_image_url'] = featured_image_url\n",
    "\n",
    "    ######## Retrieve page with the requests module\n",
    "    response_weather = requests.get(weather_url)\n",
    "    # Create BeautifulSoup object; parse with 'lxml'\n",
    "    soup_weather = BeautifulSoup(response_weather.text, 'html.parser')\n",
    "    # retrieve tweet weather\n",
    "    mars_weather = soup_weather.find('p', class_=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\").text\n",
    "    # remove trailing characters\n",
    "    mars_weather = mars_weather[:-26]\n",
    "\n",
    "    mars_dict['mars_weather'] = mars_weather\n",
    "\n",
    "    ####### get mars facts\n",
    "    mars_facts_table = pd.read_html(facts_url)\n",
    "    # take the first table\n",
    "    facts_df = mars_facts_table[0]\n",
    "    # add columns\n",
    "    facts_df.columns = ['category', 'value']\n",
    "    # set index to category\n",
    "    facts_df.set_index('category', inplace=True)\n",
    "    # turn table back into html\n",
    "    facts_html_table = facts_df.to_html()\n",
    "\n",
    "    mars_dict['facts_html_table'] = facts_html_table\n",
    "\n",
    "    link_list = []\n",
    "    photo_url = []\n",
    "    title = []\n",
    "\n",
    "    # get URLs of hemispheres\n",
    "    response_mars = requests.get(usgs_url)\n",
    "    soup_mars = BeautifulSoup(response_mars.text, 'html.parser')\n",
    "    data = soup_mars.findAll('div', class_=\"item\")\n",
    "    for div in data:\n",
    "        links = div.findAll('a')\n",
    "        for a in links:\n",
    "            link_list.append(\"https://astrogeology.usgs.gov/\" + a['href'])\n",
    "\n",
    "    for i in link_list:\n",
    "        response_X = requests.get(i)\n",
    "        soup_x = BeautifulSoup(response_X.text, 'html.parser')\n",
    "\n",
    "        for div in soup_x.findAll('div',class_='downloads'):\n",
    "                a = div.findAll('a')[0]\n",
    "                photo_url.append(a.attrs['href'])\n",
    "\n",
    "        for div in soup_x.findAll('div',class_='content'):\n",
    "                b = div.findAll('h2')[0]\n",
    "                title.append(b.text)\n",
    "\n",
    "    hemisphere_image_urls = []\n",
    "\n",
    "    # create list of dictionaries\n",
    "    for i, j in zip(title, photo_url):\n",
    "        hemisphere_image_urls.append({\"title\": i, \"photo_url\": j})\n",
    "\n",
    "    mars_dict['hemisphere_image_urls'] = hemisphere_image_urls\n",
    "\n",
    "    browser.quit()\n",
    "\n",
    "    return mars_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
